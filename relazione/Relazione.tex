\documentclass[12pt,openany]{report}
\def\tit{DNA Sequence}
\date{\today}
\makeatletter
\let\datename\@date
\makeatother
\def\authorname{Simone Lidonnici - (2061343)\\Marco Casu - (2046212)}



%Include------------------------------------------------
\usepackage[italian]{babel}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage[paper=a4paper,left=20mm,right=20mm,bottom=25mm,top=25mm]{geometry}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage[listings,breakable]{tcolorbox}
\tcbuselibrary{skins}
\usepackage{fancyhdr}
\usepackage[absolute,overlay]{textpos}
%-------------------------------------------------------




%Stile pagina
\raggedbottom
\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{\ifnum\value{chapter}>0{\nouppercase{\rightmark}}\fi}
\fancyfoot[C]{\thepage}
%------------------------------------------------


\renewcommand{\thesection}{\arabic{section}}
\definecolor{Sapienza}{RGB}{131,31,48}


\begin{document}
%INIZIO PRIMA PAGINA
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{images/Sapienza_logo.png}
    \end{center}
    \centering\Large \textbf{\color{Sapienza}{Facoltà di Ingegneria dell'Informazione, Informatica e Statistica\\Dipartimento di Informatica}}
    \vspace{4cm}
    \begin{tcolorbox}[enhanced, width=\textwidth, colframe=Sapienza, colback=white, halign=flush center, sharp corners=all, boxrule=1mm, bottom=5mm, top=5mm]
        \Huge\textbf{\tit}
    \end{tcolorbox}
    \begin{textblock*}{\textwidth}[0.5,0](0.5\pdfpagewidth,20cm)
        \centering\large\textbf{Autori:}\\\authorname
    \end{textblock*}
    \vfill
    \centering\large\datename
\end{titlepage}
%FINE PRIMA PAGINA

\section{Introduzione}
Le parti principali di codice da parallelizzare sono la creazione della sequenza e la ricerca dei pattern. Per sequenze medio-piccole quest'ultima compone la maggior parte del tempo di compilazione, mentre aumentando la grandezza della sequenza (nell'ordine dei miliardi), la maggior parte del tempo è richiesto per generare la sequenza. Da notare che nel programma sequenziale il tempo non conta la generazione di quest'ultima mentre nei programmi paralleli (MPI, OpenMP, CUDA e MPI+OpenMP) si, cosa che causa una diminuzione dello speed up.
\subsection{Test e calcolo dello speed up}
Le varie versioni del programma sono state testate sul cluster eseguendo 10 test e controllando il tempo medio di essi, scartando dal calcolo della media il caso peggiore ed il caso migliore, per evitare che la media venga modificata troppo da esecuzioni con tempi molto più alti o bassi rispetto alle altre. In particolare il test è stato eseguito con i seguenti parametri:
\begin{center}
    \texttt{500000 0.35 0.2 0.25 30000 2000 1000 30000 2000 1000 500 100 M 4353435}
\end{center}
L'esecuzione del codice sequenziale ha richiesto 71.27 secondi.

\newpage
\section{MPI}
Nel programma MPI la ricerca dei pattern è stata distribuita uniformemente tra i rank, se $t$ è il numero dei processi, allora ognuno ricercherà $\lceil \frac{n}{t}\rceil$ pattern, con $n$ numero totale di pattern (sia sample che random). Se $\frac{n}{t}$ non è un numero intero, l'ultimo rank ricercherà meno pattern rispetto ad ogni altro. Per avere poi il valore corretto dei pattern trovati e dei pattern in ogni punto della sequenza vengono eseguite delle collettive \texttt{MPI\_Reduce} su \texttt{seq\_matches} e \texttt{pat\_found}.

\bigskip 
Riguardo la generazione della sequenza,
è stato osservato che, questa, risulta  più veloce nella versione sequenziale, anche se viene adoperato un singolo thread, la funzione\\ \texttt{generate\_random\_sequence} impiega sempre meno tempo ad essere eseguita nella versione sequenziale piuttosto che in quella parallela. Sono stati tentati due approcci:
\begin{itemize}
    \item Si è inizialmente provato a far generare l'intera sequenza ad un solo processo, per poi eseguire una \texttt{MPI\_Broadcast}, condividendola a tutti gli altri processi. Tale versione si è rivelata più veloce della versione iniziale (in cui ogni rank genera autonomamente la sequenza) ma comunque più lenta della versione sequenziale.
    \item La seconda opzione, presente nel file finale, è stata quella di dividere la sequenza tra i vari rank, nello stesso modo in cui vengono divisi i pattern e poi eseguire una \texttt{MPI\_Allreduce} per far avere a tutti i rank la sequenza completa. Data la natura puramente sequenziale delle funzioni rng, ogni rank esegue un \texttt{rng\_skip} per poter iniziare a generare i suoi numeri random da un punto avanzato della sequenza.
\end{itemize}
Di seguito si può vedere il grafico dei tempi e dello speed up in relazione al numero di processi MPI, i test sono stati eseguiti con il numero massimo di processi su un nodo (32) e aumentando i nodi progressivamente:
\begin{center}
    \includegraphics[width=0.48\textwidth ]{images/tempi_MPI.pdf}
    \includegraphics[width=0.48\textwidth ]{images/speedup_MPI.pdf}
\end{center}

\newpage
Se si vogliono consultare le informazioni in maniera più pratica, si può controllare la seguente tabella:
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \rowcolor[HTML]{EFEFEF} 
        numero processi & tempo   & speed up & efficienza   \\ \hline
        sequenziale      & 71.27 & 1         & 1 \\ \hline
        32               & 2.71  & 26.27 & 0.82 \\ \hline
        64               & 1.57   & 45.07 & 0.70 \\ \hline
        96               & 1.10  & 64.64 & 0.67 \\ \hline
        128              & 0.83  & 84.97 & 0.66 \\ \hline
        160              & 0.94  & 75.03 & 0.46 \\ \hline
        192              & 0.68    & 103.28 & 0.53 \\ \hline
        224              & 1.06  & 66.95 & 0.29 \\ \hline
        256              & 1.16  & 61.32  & 0.23 \\ \hline
    \end{tabular}
\end{center}
\textbf{Nota} : il tempo medio per l'esecuzione con 256 rank risulta maggiore al tempo medio con 96 rank, anche se il caso migliore risulta più rapido, non è chiaro se l'aleatoreità delle differenze di tempo fra differenti esecuzioni del programma sia dovuta al sovraccarico del cluster. In seguito è riportato il grafico dei tempi nei \textit{casi migliori}:
\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/tempi_MPI_bestcase.pdf}
\end{center}
Si noti come nel caso migliore, all'aumentare dei thread il tempo va sempre diminuendo.



\newpage
\section{OpenMP}
Nel programma OpenMP per quanto riguarda la ricerca dei pattern abbiamo creato delle variabili \texttt{pat\_matches} e \texttt{seq\_matches} private per ogni thread e abbiamo parallelizzato solamente il ciclo più esterno, tramite un \texttt{\#pragma omp for}, questo perché i tre cicli non potevano essere collassati in uno unico data la presenza di \texttt{break} al loro interno.
\bigskip

Dopo il ciclo \texttt{for} relativo alla ricerca dei pattern ogni thread avrà operato sulle proprie copie private di \texttt{pat\_matches} e \texttt{seq\_matches}. Sarà necessario collossare i risultati parziali ottenuti da ogni thread, sommandone i valori.
A tal proposito, sarà necessario eseguire una porzione di codice in modo sequenziale con la direttiva \texttt{\#pragma omp critical}, in cui ogni thread sommerà i propri valori di \texttt{pat\_matches} e \texttt{seq\_matches} per avere i valori finali corretti.\bigskip

Per quanto riguarda invece la creazione della sequenza abbiamo usato lo stesso approccio di MPI, cioè dividerla in parti uguali in base all'id del thread e far eseguire ad ogni thread \texttt{rng\_skip} fino al punto dove deve iniziare a generare i propri numeri random. Per impostare il numero di thread è stato aggiunto un argomento in input, in modo da poterli impostare da linea di comando (l'argomento aggiuntivo è utile solo per i test, non modifica la logica del codice).\bigskip


Di seguito si può vedere il grafico dei tempi e dello speed up in relazione al numero di thread, i test sono stati eseguiti aumentando il numero di thread progressivamente:
\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/tempi_OpenMP.pdf}\includegraphics[width=0.5\textwidth ]{images/speedup_OpenMP.pdf}
\end{center}
Si osservi come la decrescita del tempo impiegato a terminare il programma è inizialmente esponenziale. In seguito, è riportata la tabella dei tempi e dello speed up:
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \rowcolor[HTML]{EFEFEF} 
        numero thread & tempo & speed up & efficienza\\ \hline
        sequenziale       & 71.27 & 1       & 1 \\ \hline
        2                 & 36.94 & 1.92    & 0.96 \\ \hline
        4                 & 18.62 & 3.82    & 0.95 \\ \hline
        8                 & 9.48  & 7.51    & 0.93 \\ \hline
        16                & 4.84  & 14.71   & 0.91 \\ \hline
        32                & 2.93  & 24.24   & 0.75 \\ \hline
        \end{tabular}
\end{center}
Si noti come a parità di processi (nel caso con 32 thread), l'esecuzione parallela in MPI risulta più efficiente rispetto quella in OpenMP.


\newpage 
\section{CUDA}
In CUDA, la logica della parallelizzazione per quanto riguarda la ricerca dei pattern è la medesima, si presenta però un problema non di poco conto:
\begin{itemize}
    \item Nelle implementazioni MPI ed OpenMP, ogni thread possiede una copia privata degli array \texttt{seq\_matches} e \texttt{pat\_found}. Il numero dei thread/processo MPI è nell'ordine delle centinaia, quindi, ciò non causa problemi dal punto di vista della memoria anche quando gli array sono di grandi dimensioni, inoltre, essendo eseguiti sulla CPU, c'è la possibilità di fare paginazione.
    \item Nel caso di CUDA, i CUDAcores sono nell'ordine delle migliaia, e la memoria della GPU è limitata, quindi in caso di grandi dimensioni per gli array prima menzionati, vi è il rischio che la memoria venga esaurita.
\end{itemize} 
A tal proposito, piuttosto che fornire ad ogni thread una versione locale di  \texttt{seq\_matches} e \texttt{pat\_found}, è possibile avere una versione globale condivisa, su cui la scrittura deve essere resa sequenziale per evitare race condition. È stata tentata un'implementazione di questo tipo, ma la sequenzializzazione della scrittura comporta un'abbassamento delle performance notevole, rendendo il programma ben più lento della versione sequenziale.\bigskip 

È stata quindi adoperata la versione in cui ogni thread ha una versione privata degli array, dato che la memoria disponibile è sufficiente a permettere l'esecuzione del programma sugli input utilizzati per i test da noi adottati.\bigskip 

Di seguito si può vedere il grafico dei tempi e dello speed up in relazione al numero di thread, i test sono stati eseguiti aumentando progressivamente il numero di blocchi nella griglia, mantenendo costante il numero di thread nel blocco (precisamente, 64):
\begin{center}
    \includegraphics[width=0.42\textwidth ]{images/tempiCUDA.pdf}
    \includegraphics[width=0.53\textwidth ]{images/speedupCUDA.pdf}
\end{center}\newpage
Come nel caso di MPI, ad un certo punto l'aggiunta di thread non causa alcun beneficio nelle performance. Se si vogliono consultare le informazioni in maniera più pratica, si può controllare la seguente tabella:
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        \rowcolor[HTML]{EFEFEF} 
        numero thread & tempo     & speed up \\ \hline
        sequenziale      & 71.27     & 1       \\ \hline
        256              & 40.43     & 1.76    \\ \hline
        512              & 14.31     & 4.98    \\ \hline
        1024             & 11.99     & 5.98    \\ \hline
        2048             & 6.58      & 10.83   \\ \hline
        4608             & 8.16      & 8.73    \\ \hline
        \end{tabular}
\end{center}
\newpage 
\section{MPI + OpenMP}
Per l'implementazione mista, abbiamo adottato la seguente procedura:
\begin{itemize}
    \item Ogni nodo sul cluster avvia un processo MPI.
    \item Per ogni nodo, quindi per ogni processo MPI, ci sono più thread openMP (che verranno parallelamente schedulati sui diversi core del processore presente sul nodo).
\end{itemize}
Su ogni nodo, i thread condivideranno una versione locale di \texttt{pat\_found}, le riduzioni quindi, avverranno fra nodi differenti. Si noti come:
\begin{itemize}
    \item Nella versione MPI, se ci sono 32 rank, la riduzione coinvolgerà 32 array.
    \item Nella versione MPI+OpenMP, ad esempio, con 8 rank e 32 thread per processo, la riduzione coinvolgerà solamente 8 array (presenti sui differenti nodi).
\end{itemize}
Sono state eseguite due serie differenti di test:
\begin{itemize}
    \item Nella prima serie, si è lasciato fisso il numero di thread OpenMP a 32, e si è aumentato progressivamente il numero di nodi (rank MPI).
    \item Nella seconda serie, sono stati fissati 8 nodi (rank MPI) e sono stati variati i thread OpenMP.
\end{itemize}
La progressione del numero dei thread totali è identica fra le due serie. In seguito sono riportati i grafici dei tempi delle due serie:
\begin{center}
    \includegraphics[width=0.48\textwidth ]{images/tempi_MISTO_M.pdf}
    \includegraphics[width=0.48\textwidth ]{images/tempi_MISTO_T.pdf}
\end{center}
I due grafici a confronto, in blu la seconda serie, in arancione la prima:
\begin{center}
    \includegraphics[width=0.48\textwidth ]{images/tempi_MISTO_MT.pdf}
\end{center}
È riportato ora il grafico dello speed up:
\begin{center}
    \includegraphics[width=0.48\textwidth ]{images/speedup_MISTO_M.pdf}
    \includegraphics[width=0.48\textwidth ]{images/speedup_MISTO_T.pdf}
\end{center}
I due grafici a confronto, in blu la seconda serie, in arancione la prima:
\begin{center}
    \includegraphics[width=0.48\textwidth ]{images/speedup_MISTO_MT.pdf}
\end{center}
È riportata la tabella dei tempi e dello speed up:
\begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \rowcolor[HTML]{EFEFEF} 
        numero thread & tempo 1 & tempo 2 & speed up 1 & speed up 2 & efficienza 1 & efficienza 2\\ \hline
        sequenziale          & 71.27      & 71.27                              & 1                 & 1       & 1 &1          \\ \hline
        32                   & 3.44       & 2.53                               & 20.70         & 28.09   & 0.64&0.87      \\ \hline
        64                   & 1.80       & 1.32                              & 39.41         & 53.65    &0.61&0.83      \\ \hline
        96                   & 1.26       & 1.00                              & 56.37         & 71.25    & 0.58&0.74     \\ \hline
        128                  & 1.08       & 0.84                               & 65.45         & 84.81   &0.51&0.66      \\ \hline
        160                  & 0.85       & 0.74                               & 83.44         & 95.20   &0.52&0.59      \\ \hline
        192                  & 0.75       & 0.66                               & 94.37         & 106.58  &0.49&0.55      \\ \hline
        224                  & 0.69       & 0.75                               & 102.07        & 94.79   &0.45&0.42      \\ \hline
        256                  & 0.64       & 0.64                            & 110.16        & 110.16     &0.43&0.43   \\ \hline
        \end{tabular}
\end{center}

Possiamo notare quindi che in media l'implementazione più veloce a parità del numero di thread totali sia MPI+OpenMP.
\end{document}